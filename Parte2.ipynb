{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n",
      "Loss: 0.40071040519908424\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-51a16a817b53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "class gaussian:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return np.exp(-z**2)\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        return -2 * z * np.exp(-z**2)\n",
    "    \n",
    "class tanh: \n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return np.tanh(z)\n",
    "    def prime(z):\n",
    "        return 1.0 - np.tanh(z)**2\n",
    "\n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        \"\"\"\n",
    "        :param activation_fn: Class object of the activation function.\n",
    "        \"\"\"\n",
    "        if activation_fn:\n",
    "            self.activation_fn = activation_fn\n",
    "        else:\n",
    "            self.activation_fn = NoActivation\n",
    "\n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :param y_pred: (array) Prediction vector\n",
    "        :return: (flt)\n",
    "        \"\"\"\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Back propagation error delta\n",
    "        :return: (array)\n",
    "        \"\"\"\n",
    "        return self.prime(y_true, y_pred) * self.activation_fn.prime(y_pred)\n",
    "\n",
    "\n",
    "class NoActivation:\n",
    "    \"\"\"\n",
    "    This is a plugin function for no activation.\n",
    "    f(x) = x * 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        \"\"\"\n",
    "        :param z: (array) w(x) + b\n",
    "        :return: z (array)\n",
    "        \"\"\"\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        \"\"\"\n",
    "        The prime of z * 1 = 1\n",
    "        :param z: (array)\n",
    "        :return: z': (array)\n",
    "        \"\"\"\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimensions, activations):\n",
    "        \"\"\"\n",
    "        :param dimensions: (tpl/ list) Dimensions of the neural net. (input, hidden layer, output)\n",
    "        :param activations: (tpl/ list) Activations functions.\n",
    "        Example of one hidden layer with\n",
    "        - 2 inputs\n",
    "        - 3 hidden nodes\n",
    "        - 3 outputs\n",
    "        layers -->    [1,        2,          3]\n",
    "        ----------------------------------------\n",
    "        dimensions =  (2,     3,          3)\n",
    "        activations = (      Relu,      Sigmoid)\n",
    "        \"\"\"\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "\n",
    "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "\n",
    "    def _feed_forward(self, x):\n",
    "        \"\"\"\n",
    "        Execute a forward feed through the network.\n",
    "        :param x: (array) Batch of input data vectors.\n",
    "        :return: (tpl) Node outputs and activations per layer. The numbering of the output is equivalent to the layer numbers.\n",
    "        \"\"\"\n",
    "\n",
    "        # w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # activations: f(z)\n",
    "        a = {1: x}  # First layer has no activations as input. The input x is the input.\n",
    "\n",
    "        for i in range(1, self.n_layers):\n",
    "            # current layer = i\n",
    "            # activation layer = i + 1\n",
    "            z[i + 1] = np.dot(a[i], self.w[i]) + self.b[i]\n",
    "            a[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _back_prop(self, z, a, y_true):\n",
    "        \"\"\"\n",
    "        The input dicts keys represent the layers of the net.\n",
    "        a = { 1: x,\n",
    "              2: f(w1(x) + b1)\n",
    "              3: f(w2(a2) + b2)\n",
    "              }\n",
    "        :param z: (dict) w(x) + b\n",
    "        :param a: (dict) f(z)\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine partial derivative and delta for the output layer.\n",
    "        # delta output layer\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # In case of three layer net will iterate over i = 2 and i = 1\n",
    "        # Determine partial derivative and delta for the rest of the layers.\n",
    "        # Each iteration requires the delta from the previous layer, propagating backwards.\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.w[i].T) * self.activations[i].prime(z[i])\n",
    "            dw = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (dw, delta)\n",
    "\n",
    "        for k, v in update_params.items():\n",
    "            self._update_w_b(k, v[0], v[1])\n",
    "\n",
    "    def _update_w_b(self, index, dw, delta):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        :param index: (int) Number of the layer\n",
    "        :param dw: (array) Partial derivatives\n",
    "        :param delta: (array) Delta error.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w[index] -= self.learning_rate * dw\n",
    "        self.b[index] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_true, loss, epochs, batch_size, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :param y_true: (array) Containing one hot encoded labels.\n",
    "        :param loss: Loss class (MSE, CrossEntropy etc.)\n",
    "        :param epochs: (int) Number of epochs.\n",
    "        :param batch_size: (int)\n",
    "        :param learning_rate: (flt)\n",
    "        \"\"\"\n",
    "        if not x.shape[0] == y_true.shape[0]:\n",
    "            raise ValueError(\"Length of x and y arrays don't match\")\n",
    "        # Initiate the loss object with the final activation function\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self._feed_forward(x_[k:l])\n",
    "                self._back_prop(z, a, y_[k:l])\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                _, a = self._feed_forward(x)\n",
    "                print(\"Loss:\", self.loss.loss(y_true, a[self.n_layers]))\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :return: (array) A 2D array of shape (n_cases, n_classes).\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(x)\n",
    "        return a[self.n_layers]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn import datasets\n",
    "    import pandas as pd\n",
    "    path = pd.read_csv('Data_Balanceada.csv') \n",
    " \n",
    "    \n",
    "    x = path.iloc[:,0:11].values\n",
    "    y = path.iloc[:,11].values\n",
    "\n",
    "    nn = Network((11, 5, 1), (tanh, Sigmoid))\n",
    "    nn.fit(x, y, loss=MSE, epochs=500, batch_size=100, learning_rate=1e-3)\n",
    "\n",
    "    prediction = nn.predict(x)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in range(len(y)):\n",
    "        y_pred.append(np.argmax(prediction[i]))\n",
    "        y_true.append(np.argmax(y[i]))\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
